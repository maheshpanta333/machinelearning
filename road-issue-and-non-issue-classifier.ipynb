{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T08:44:50.721679Z",
     "iopub.status.busy": "2025-12-20T08:44:50.721092Z",
     "iopub.status.idle": "2025-12-20T08:45:57.558939Z",
     "shell.execute_reply": "2025-12-20T08:45:57.558063Z",
     "shell.execute_reply.started": "2025-12-20T08:44:50.721651Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip uninstall -y protobuf tensorflow\n",
    "%pip install protobuf==3.20.3 tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for bigger project of ChitraDartaa where we classify civic issue for which this is an initial CNN model that takes image and classifies if the image has issue or not specifially checks if it is an outdoor image with an issue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-20T11:39:38.382456Z",
     "iopub.status.busy": "2025-12-20T11:39:38.381668Z",
     "iopub.status.idle": "2025-12-20T11:39:52.245670Z",
     "shell.execute_reply": "2025-12-20T11:39:52.244847Z",
     "shell.execute_reply.started": "2025-12-20T11:39:38.382416Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Sequential\n",
    "from tensorflow.keras.utils import image_dataset_from_directory, load_img, img_to_array\n",
    "import zipfile\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T11:39:52.247300Z",
     "iopub.status.busy": "2025-12-20T11:39:52.246855Z",
     "iopub.status.idle": "2025-12-20T11:39:52.250985Z",
     "shell.execute_reply": "2025-12-20T11:39:52.250332Z",
     "shell.execute_reply.started": "2025-12-20T11:39:52.247275Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#setting up important var for loading images later on\n",
    "BATCH_SIZE=32\n",
    "IMG_SIZE=(224,224)\n",
    "SEED=42\n",
    "EXTRACT_PATH=\"/kaggle/input/civic-issue-dataset/Dataset\"\n",
    "selected_class=[\"NoIssue\",\"Issues\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T11:39:52.252470Z",
     "iopub.status.busy": "2025-12-20T11:39:52.252195Z",
     "iopub.status.idle": "2025-12-20T11:40:38.017209Z",
     "shell.execute_reply": "2025-12-20T11:40:38.016690Z",
     "shell.execute_reply.started": "2025-12-20T11:39:52.252446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#loading up images and dividing them for the neural net\n",
    "train_data=tf.keras.utils.image_dataset_from_directory(\n",
    "    EXTRACT_PATH,\n",
    "    class_names=selected_class,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode=\"binary\" \n",
    ")\n",
    "\n",
    "#validation dataset\n",
    "validation_data=tf.keras.utils.image_dataset_from_directory(\n",
    "    EXTRACT_PATH,\n",
    "    class_names=selected_class,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode=\"binary\"\n",
    ")\n",
    "#now dividing the validation data set into \n",
    "total_number_of_batches_in_validation_data=validation_data.cardinality().numpy() #converts total number of batches in the set and converts the tensor into python integer with .numpy function here\n",
    "no_of_batches_in_validation_data=total_number_of_batches_in_validation_data//2 #this is a floor division operator\n",
    "\n",
    "\n",
    "#now from the set of batches of the image creating subset into the validation and the test subset\n",
    "validation=validation_data.take(no_of_batches_in_validation_data)\n",
    "test=validation_data.skip(no_of_batches_in_validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T11:40:38.018830Z",
     "iopub.status.busy": "2025-12-20T11:40:38.018582Z",
     "iopub.status.idle": "2025-12-20T11:40:38.023101Z",
     "shell.execute_reply": "2025-12-20T11:40:38.022373Z",
     "shell.execute_reply.started": "2025-12-20T11:40:38.018808Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Class names: {train_data.class_names}\")\n",
    "print(\"For values:\\n\")\n",
    "for i, class_name in enumerate(train_data.class_names):\n",
    "    print(f\"{class_name}:{i}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T11:40:38.024147Z",
     "iopub.status.busy": "2025-12-20T11:40:38.023888Z",
     "iopub.status.idle": "2025-12-20T11:59:50.979493Z",
     "shell.execute_reply": "2025-12-20T11:59:50.978916Z",
     "shell.execute_reply.started": "2025-12-20T11:40:38.024125Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#to augment data which we should do while training \n",
    "data_augmentation=tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.15),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "    tf.keras.layers.RandomContrast(0.1),\n",
    "    tf.keras.layers.GaussianNoise(0.05),\n",
    "    tf.keras.layers.RandomTranslation(0.1,0.1),\n",
    "    tf.keras.layers.RandomCrop(224,224),\n",
    "    tf.keras.layers.RandomBrightness(factor=0.2),\n",
    "    tf.keras.layers.RandomSaturation(0.05),\n",
    "    tf.keras.layers.RandomFlip(\"vertical\"),\n",
    "])\n",
    "\n",
    "#setting var to send for training to send info about input\n",
    "input=tf.keras.Input(shape=(224,224,3))\n",
    "\n",
    "\n",
    "#now the acutal model which would be we want conv2d->pooling->conv2d->pooling->conv2d_>pooling-> flatten all the data two dense layer with relu\n",
    "model=tf.keras.Sequential([\n",
    "    input,\n",
    "    data_augmentation,\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "\n",
    "#CNN LAYERS\n",
    "    tf.keras.layers.Conv2D(32,3,padding='same',activation=\"relu\"),\n",
    "    #here 32 is the number of filters/kernels, padding is same so it extends for the edges with 0 values and activation we chose is relu\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "\n",
    "    tf.keras.layers.Conv2D(64,3, padding=\"same\",activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(128,3,padding=\"same\",activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "\n",
    "    #now flatten this cube of input\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.5), #this makes the neural net to ignore 50 percent of the units all of the time so it would not look for same type of data everytime and prevent overfitting\n",
    "    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "])\n",
    "#now compile and fit them\n",
    "model.compile(optimizer=\"adam\",\n",
    "             loss=BinaryCrossentropy(from_logits=False),\n",
    "             metrics=[\"accuracy\",\n",
    "             tf.keras.metrics.Precision(name=\"Precision\"),\n",
    "             tf.keras.metrics.Recall(name=\"recall\"),])\n",
    "history=model.fit(\n",
    "    train_data,\n",
    "    validation_data=validation,\n",
    "    epochs=100,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_accuracy\", #checks for val_accuracy\n",
    "            patience=5,#wait tills 5 epochs\n",
    "            restore_best_weights=True,#uses best weight\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            \"best_model.keras\",#givesbest model according to val_accuracy\n",
    "            monitor=\"val_accuracy\",\n",
    "            save_best_only=True,\n",
    "            verbose=1 #prints only certain line of epoch for 1 and for 0 is silence and for 2 is every line\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "test_loss,test_acc,test_precision,test_recall=model.evaluate(validation_data)\n",
    "print(f\"Accuracy: {test_acc:4f}, Precision:{test_precision:.4f}, Recall:{test_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For accuracy:\n",
    "1) We have total of 11,252 images of issues and 3954 images of non-issues and total of 15,206 images.\n",
    "2) For threshold accuracy we take a very dumb modal that would always predict the class with higher images so which would be:\n",
    "       threshold_accuracy=11252/15206*100=73.99%\n",
    "3) However, we do need to figure out recall and precision values which we take harmonic mean of must be >=0.65\n",
    "   NOTE: Why do we take recall and precision here?\n",
    "   => We do it because we have unbalanced data not only we need accurate value we need values with good precision and recall value where,<br>\n",
    "   precision=(True positive values(values we predicted positive and is positive))/(No of True values we predict(both false and true positive))<br>\n",
    "   recall=(true +ve(no of predicted=actual true)/Total actual positive value(both true positive we predict and false negative we predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T11:59:50.981067Z",
     "iopub.status.busy": "2025-12-20T11:59:50.980574Z",
     "iopub.status.idle": "2025-12-20T11:59:51.611465Z",
     "shell.execute_reply": "2025-12-20T11:59:51.610689Z",
     "shell.execute_reply.started": "2025-12-20T11:59:50.981043Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"/kaggle/working/models\",exist_ok=True)\n",
    "model.save(\"/kaggle/working/models/pothole_model.keras\")"
   ]
  },
  {
   "attachments": {
    "007414ef-54bd-458c-9b2a-540a8c7e09ef.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:007414ef-54bd-458c-9b2a-540a8c7e09ef.png)\n",
    "\n",
    "This is for cv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T11:59:51.612692Z",
     "iopub.status.busy": "2025-12-20T11:59:51.612373Z",
     "iopub.status.idle": "2025-12-20T12:00:01.733313Z",
     "shell.execute_reply": "2025-12-20T12:00:01.732765Z",
     "shell.execute_reply.started": "2025-12-20T11:59:51.612660Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result=model.evaluate(test)\n",
    "print(f\"For the unseen test data of the entire training this model has accuracy of {result[1]:.4f}, precision of {result[2]:.4f}, and recall of {result[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harmonic mean we got is 0.939 which is a very good value for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T12:03:01.139389Z",
     "iopub.status.busy": "2025-12-20T12:03:01.137834Z",
     "iopub.status.idle": "2025-12-20T12:03:01.150546Z",
     "shell.execute_reply": "2025-12-20T12:03:01.148960Z",
     "shell.execute_reply.started": "2025-12-20T12:03:01.139320Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_for_single_image(model, img_path):\n",
    "    img=tf.keras.utils.load_img(img_path, target_size=(224,224))\n",
    "    img_array=tf.keras.utils.img_to_array(img)\n",
    "    img_array=np.expand_dims(img_array, axis=0)\n",
    "    prediction=model.predict(img_array)\n",
    "    probability=prediction[0][0]\n",
    "    if probability>0.5:\n",
    "        return 1;\n",
    "    return 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T12:05:46.891920Z",
     "iopub.status.busy": "2025-12-20T12:05:46.891592Z",
     "iopub.status.idle": "2025-12-20T12:05:47.718591Z",
     "shell.execute_reply": "2025-12-20T12:05:47.718047Z",
     "shell.execute_reply.started": "2025-12-20T12:05:46.891892Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#to load up images\n",
    "print(f\"For i1 we got {predict_for_single_image(model, \"/kaggle/input/helpppp/helppp/i1.png\")}\")\n",
    "print(f\"For i11 we got {predict_for_single_image(model, \"/kaggle/input/helpppp/helppp/i11.jpeg\")}\")\n",
    "print(f\"For i2 we got {predict_for_single_image(model, \"/kaggle/input/helpppp/helppp/i2.png\")}\")\n",
    "print(f\"For i3 we got {predict_for_single_image(model, \"/kaggle/input/helpppp/helppp/i3.png\")}\")\n",
    "print(f\"For n1 we got {predict_for_single_image(model, \"/kaggle/input/helpppp/helppp/n1.jpg\")}\")\n",
    "print(f\"For n2 we got {predict_for_single_image(model, \"/kaggle/input/helpppp/helppp/n2.jpg\")}\")\n",
    "print(f\"For n3 we got {predict_for_single_image(model, \"/kaggle/input/helpppp/helppp/n3.png\")}\")\n",
    "print(f\"For n4 we got {predict_for_single_image(model, \"/kaggle/input/helpppp/helppp/n4.jpeg\")}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T12:11:45.662967Z",
     "iopub.status.busy": "2025-12-20T12:11:45.662319Z",
     "iopub.status.idle": "2025-12-20T12:11:54.504828Z",
     "shell.execute_reply": "2025-12-20T12:11:54.504130Z",
     "shell.execute_reply.started": "2025-12-20T12:11:45.662937Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Zip the folder for easy download\n",
    "shutil.make_archive(\"/kaggle/working/models\", 'zip', \"/kaggle/working/models\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9079827,
     "sourceId": 14232419,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9081886,
     "sourceId": 14235341,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
