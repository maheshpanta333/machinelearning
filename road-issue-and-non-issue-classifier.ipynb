{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14235341,"sourceType":"datasetVersion","datasetId":9081886},{"sourceId":14346697,"sourceType":"datasetVersion","datasetId":9160474},{"sourceId":694825,"sourceType":"modelInstanceVersion","modelInstanceId":526911,"modelId":540960}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y protobuf tensorflow\n!pip install protobuf==3.20.3 tensorflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T16:04:35.596706Z","iopub.execute_input":"2025-12-31T16:04:35.596901Z","iopub.status.idle":"2025-12-31T16:05:32.334592Z","shell.execute_reply.started":"2025-12-31T16:04:35.596879Z","shell.execute_reply":"2025-12-31T16:05:32.333858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, Sequential\nfrom tensorflow.keras.utils import image_dataset_from_directory, load_img, img_to_array\nimport zipfile\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nimport os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T16:05:32.336320Z","iopub.execute_input":"2025-12-31T16:05:32.336838Z","iopub.status.idle":"2025-12-31T16:05:39.949116Z","shell.execute_reply.started":"2025-12-31T16:05:32.336808Z","shell.execute_reply":"2025-12-31T16:05:39.948575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#setting up important var for loading images later on\nBATCH_SIZE=32\nIMG_SIZE=(224,224)\nSEED=42\nEXTRACT_PATH=\"/kaggle/input/civic-issue-dataset/Dataset\"\nselected_class=[\"NoIssue\",\"Issues\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T16:05:39.949975Z","iopub.execute_input":"2025-12-31T16:05:39.950394Z","iopub.status.idle":"2025-12-31T16:05:39.954489Z","shell.execute_reply.started":"2025-12-31T16:05:39.950354Z","shell.execute_reply":"2025-12-31T16:05:39.953419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#loading up images and dividing them for the neural net\ntrain_data=tf.keras.utils.image_dataset_from_directory(\n    EXTRACT_PATH,\n    class_names=selected_class,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=SEED,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    label_mode=\"binary\" \n)\n\n#validation dataset\nvalidation_data=tf.keras.utils.image_dataset_from_directory(\n    EXTRACT_PATH,\n    class_names=selected_class,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=SEED,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    label_mode=\"binary\"\n)\n#now dividing the validation data set into \ntotal_number_of_batches_in_validation_data=validation_data.cardinality().numpy() #converts total number of batches in the set and converts the tensor into python integer with .numpy function here\nno_of_batches_in_validation_data=total_number_of_batches_in_validation_data//2 #this is a floor division operator\n\n\n#now from the set of batches of the image creating subset into the validation and the test subset\nvalidation=validation_data.take(no_of_batches_in_validation_data)\ntest=validation_data.skip(no_of_batches_in_validation_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T16:05:39.955615Z","iopub.execute_input":"2025-12-31T16:05:39.955867Z","iopub.status.idle":"2025-12-31T16:06:25.541405Z","shell.execute_reply.started":"2025-12-31T16:05:39.955846Z","shell.execute_reply":"2025-12-31T16:06:25.540776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Class names: {train_data.class_names}\")\nprint(\"For values:\\n\")\nfor i, class_name in enumerate(train_data.class_names):\n    print(f\"{class_name}:{i}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T16:06:25.542425Z","iopub.execute_input":"2025-12-31T16:06:25.542684Z","iopub.status.idle":"2025-12-31T16:06:25.546913Z","shell.execute_reply.started":"2025-12-31T16:06:25.542662Z","shell.execute_reply":"2025-12-31T16:06:25.546155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#to augment data which we should do while training \ndata_augmentation=tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\"),\n    tf.keras.layers.RandomRotation(0.15),\n    tf.keras.layers.RandomZoom(0.1),\n    tf.keras.layers.RandomContrast(0.1),\n    tf.keras.layers.GaussianNoise(0.05),\n    tf.keras.layers.RandomTranslation(0.1,0.1),\n    tf.keras.layers.RandomCrop(224,224),\n    tf.keras.layers.RandomBrightness(factor=0.2),\n    tf.keras.layers.RandomSaturation(0.05),\n    tf.keras.layers.RandomFlip(\"vertical\"),\n])\n\n#setting var to send for training to send info about input\ninput=tf.keras.Input(shape=(224,224,3))\n\n\n#now the acutal model which would be we want conv2d->pooling->conv2d->pooling->conv2d_>pooling-> flatten all the data two dense layer with relu\nmodel=tf.keras.Sequential([\n    input,\n    data_augmentation,\n    tf.keras.layers.Rescaling(1./255),\n\n#CNN LAYERS\n    tf.keras.layers.Conv2D(32,3,padding='same',activation=\"relu\"),\n    #here 32 is the number of filters/kernels, padding is same so it extends for the edges with 0 values and activation we chose is relu\n    tf.keras.layers.MaxPooling2D(),\n\n    tf.keras.layers.Conv2D(64,3, padding=\"same\",activation=\"relu\"),\n    tf.keras.layers.MaxPooling2D(),\n    \n    tf.keras.layers.Conv2D(128,3,padding=\"same\",activation=\"relu\"),\n    tf.keras.layers.MaxPooling2D(),\n\n    #now flatten this cube of input\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=\"relu\"),\n    tf.keras.layers.Dropout(0.5), #this makes the neural net to ignore 50 percent of the units all of the time so it would not look for same type of data everytime and prevent overfitting\n    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n])\n#now compile and fit them\nmodel.compile(optimizer=\"adam\",\n             loss=BinaryCrossentropy(from_logits=False),\n             metrics=[\"accuracy\",\n             tf.keras.metrics.Precision(name=\"Precision\"),\n             tf.keras.metrics.Recall(name=\"recall\"),])\nhistory=model.fit(\n    train_data,\n    validation_data=validation,\n    epochs=100,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\", #checks for val_accuracy\n            patience=5,#wait tills 5 epochs\n            restore_best_weights=True,#uses best weight\n        ),\n        ModelCheckpoint(\n            \"best_model.keras\",#givesbest model according to val_accuracy\n            monitor=\"val_accuracy\",\n            save_best_only=True,\n            verbose=1 #prints only certain line of epoch for 1 and for 0 is silence and for 2 is every line\n        )\n    ]\n)\n\n\ntest_loss,test_acc,test_precision,test_recall=model.evaluate(validation_data)\nprint(f\"Accuracy: {test_acc:4f}, Precision:{test_precision:.4f}, Recall:{test_recall:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T16:06:25.548854Z","iopub.execute_input":"2025-12-31T16:06:25.549094Z","iopub.status.idle":"2025-12-31T16:24:22.010809Z","shell.execute_reply.started":"2025-12-31T16:06:25.549074Z","shell.execute_reply":"2025-12-31T16:24:22.010083Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For accuracy:\n1) We have total of 15.6k  images of issues and 3954 images of non-issues and total of 19549 images.\n2) For threshold accuracy we take a very dumb modal that would always predict the class with higher images so which would be:\n       threshold_accuracy=(15640/19549)*100=80.004%\n3) However, we do need to figure out recall and precision values which we take harmonic mean of must be >=0.65\n   NOTE: Why do we take recall and precision here?\n   => We do it because we have unbalanced data not only we need accurate value we need values with good precision and recall value where,<br>\n   precision=(True positive values(values we predicted positive and is positive))/(No of True values we predict(both false and true positive))<br>\n   recall=(true +ve(no of predicted=actual true)/Total actual positive value(both true positive we predict and false negative we predict))","metadata":{}},{"cell_type":"code","source":"os.makedirs(\"/kaggle/working/models\",exist_ok=True)\nmodel.save(\"/kaggle/working/models/pothole_model.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T16:24:22.011696Z","iopub.execute_input":"2025-12-31T16:24:22.011910Z","iopub.status.idle":"2025-12-31T16:24:22.649423Z","shell.execute_reply.started":"2025-12-31T16:24:22.011888Z","shell.execute_reply":"2025-12-31T16:24:22.648635Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"![image.png](attachment:007414ef-54bd-458c-9b2a-540a8c7e09ef.png)\n\nThis is for cv data","metadata":{},"attachments":{"007414ef-54bd-458c-9b2a-540a8c7e09ef.png":{"image/png":"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"}}},{"cell_type":"code","source":"result=model.evaluate(test)\nprint(f\"For the unseen test data of the entire training this model has accuracy of {result[1]:.4f}, precision of {result[2]:.4f}, and recall of {result[3]:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T16:24:22.650457Z","iopub.execute_input":"2025-12-31T16:24:22.650872Z","iopub.status.idle":"2025-12-31T16:24:34.844767Z","shell.execute_reply.started":"2025-12-31T16:24:22.650840Z","shell.execute_reply":"2025-12-31T16:24:34.843952Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Harmonic mean we got is 0.95855 which is a very good value for the data","metadata":{}},{"cell_type":"code","source":"def predict_for_single_image(model, img_path):\n    img=tf.keras.utils.load_img(img_path, target_size=(224,224))\n    img_array=tf.keras.utils.img_to_array(img)\n    img_array=np.expand_dims(img_array, axis=0)\n    prediction=model.predict(img_array)\n    probability=prediction[0][0]\n    if probability>0.5:\n        return 1;\n    return 0;","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T16:24:34.845685Z","iopub.execute_input":"2025-12-31T16:24:34.845948Z","iopub.status.idle":"2025-12-31T16:24:34.851819Z","shell.execute_reply.started":"2025-12-31T16:24:34.845925Z","shell.execute_reply":"2025-12-31T16:24:34.851118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#to load up images\nprint(f\"For i1 we got {predict_for_single_image(model, \"/kaggle/input/helpppp/helppp/i1.png\")}\")\nprint(f\"For i11 we got {predict_for_single_image(model, \"/kaggle/input/helpppp/helppp/i11.jpeg\")}\")\nprint(f\"For i2 we got {predict_for_single_image(model, \"/kaggle/input/helpppp/helppp/i2.png\")}\")\nprint(f\"For i3 we got {predict_for_single_image(model, \"/kaggle/input/helpppp/helppp/i3.png\")}\")\nprint(f\"For n1 we got {predict_for_single_image(model, \"/kaggle/input/helpppp/helppp/n1.jpg\")}\")\nprint(f\"For n2 we got {predict_for_single_image(model, \"/kaggle/input/helpppp/helppp/n2.jpg\")}\")\nprint(f\"For n3 we got {predict_for_single_image(model, \"/kaggle/input/helpppp/helppp/n3.png\")}\")\nprint(f\"For n4 we got {predict_for_single_image(model, \"/kaggle/input/helpppp/helppp/n4.jpeg\")}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T16:24:34.852714Z","iopub.execute_input":"2025-12-31T16:24:34.853023Z","iopub.status.idle":"2025-12-31T16:24:36.165017Z","shell.execute_reply.started":"2025-12-31T16:24:34.853001Z","shell.execute_reply":"2025-12-31T16:24:36.164467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Zip the folder for easy download\nshutil.make_archive(\"/kaggle/working/models\", 'zip', \"/kaggle/working/models\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T16:25:06.935284Z","iopub.execute_input":"2025-12-31T16:25:06.935660Z","iopub.status.idle":"2025-12-31T16:25:15.386502Z","shell.execute_reply.started":"2025-12-31T16:25:06.935623Z","shell.execute_reply":"2025-12-31T16:25:15.385886Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#this is to see what our model did and how it is predicting\ndef visualisation(model,dataset,num_images=32):\n    images, label=next(iter(dataset.take(1))) #take 1 takes one batch from tensorflow data set\n    #where (image, 0 or 1) and this is returned as ptr or itr and next just gives access to it\n    #following model is trained model as above\n    pred=model.predict(images)\n    plt.figure(figsize=(90,80))#image with 15 inches height and 10 inches width\n\n    #randomly taking images\n    indices=np.random.choice(range(len(images)),num_images,replace=False)\n    #this randomly takes images from total number of images with replace=false so no duplicate images will be passes\n    \n\n    for i, idx in enumerate(indices):\n        ax=plt.subplot(8,4,i+1) #creating subplot of 8*4 grid\n        #converting images into the original scale as we rescale it into the image\n        plt.imshow(images[idx].numpy().astype(\"uint8\"))#displays image by converting first tensor into pixel values and then uint8 ie 0-255 \n        actual_val=int(label[idx])\n        predicted_val=1 if pred[idx]>0.5 else 0\n        color =\"green\" if actual_val==predicted_val else \"red\"\n        title=f\"Actual:{actual_val}\\nPredicted:{predicted_val}\"\n        plt.title(title,color=color,fontsize=40,fontweight=\"bold\")\n        plt.axis(\"off\")\n\n    #adding horizontal and vertical spaces between plots\n    plt.subplots_adjust(hspace=0.6,wspace=0.3)\n    plt.tight_layout\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T16:24:36.165903Z","iopub.execute_input":"2025-12-31T16:24:36.166183Z","iopub.status.idle":"2025-12-31T16:24:36.172513Z","shell.execute_reply.started":"2025-12-31T16:24:36.166152Z","shell.execute_reply":"2025-12-31T16:24:36.171826Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#seeing how model predicted for different images\nvisualisation(model,train_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T16:24:36.173449Z","iopub.execute_input":"2025-12-31T16:24:36.173679Z","iopub.status.idle":"2025-12-31T16:24:43.364743Z","shell.execute_reply.started":"2025-12-31T16:24:36.173659Z","shell.execute_reply":"2025-12-31T16:24:43.363974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualisation(model,validation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T16:24:43.365678Z","iopub.execute_input":"2025-12-31T16:24:43.365911Z","iopub.status.idle":"2025-12-31T16:24:50.103183Z","shell.execute_reply.started":"2025-12-31T16:24:43.365890Z","shell.execute_reply":"2025-12-31T16:24:50.102009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualisation(model,test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T16:24:50.104813Z","iopub.execute_input":"2025-12-31T16:24:50.105193Z","iopub.status.idle":"2025-12-31T16:25:06.933385Z","shell.execute_reply.started":"2025-12-31T16:24:50.105142Z","shell.execute_reply":"2025-12-31T16:25:06.931999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}